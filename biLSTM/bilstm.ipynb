{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Hyperlinks in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find(string):\n",
    "  \n",
    "    # findall() has been used \n",
    "    # with valid conditions for urls in string\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url = re.findall(regex,string)     \n",
    "    return [x[0] for x in url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader / Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../train.csv')\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "wordcount = defaultdict(int)\n",
    "vocab_size = 500\n",
    "\n",
    "lines = []\n",
    "maxlen = 0\n",
    "for data in train_data['Tweet']:\n",
    "\n",
    "    line = ['<START>']\n",
    "\n",
    "    tokens = tokenizer.tokenize(data)\n",
    "\n",
    "    for token in tokens:\n",
    "        url = Find(token)\n",
    "        if not url:\n",
    "            line.append(token.lower())\n",
    "            wordcount[token.lower()] += 1\n",
    "        else:\n",
    "            line.append('URL')\n",
    "            wordcount['URL'] += 1\n",
    "\n",
    "    line.append('<END>')\n",
    "    maxlen = max(maxlen, len(line))\n",
    "    lines.append(line)\n",
    "\n",
    "wordcount['<START>'] = len(train_data['Tweet'])\n",
    "wordcount['<END>'] = len(train_data['Tweet'])\n",
    "\n",
    "sorted_wordcounts = sorted(wordcount.items(), key = lambda item: item[1], reverse=True)\n",
    "\n",
    "word2ind = {}\n",
    "ind2word = {}\n",
    "\n",
    "ind = 1\n",
    "for k, v in sorted_wordcounts[:vocab_size - 1]:\n",
    "    word2ind[k] = ind\n",
    "    ind2word[ind] = k\n",
    "    ind += 1\n",
    "\n",
    "for k, v in sorted_wordcounts[vocab_size - 1:]:\n",
    "    word2ind[k] = vocab_size\n",
    "    ind2word[vocab_size - 1] = 'UKN'\n",
    "\n",
    "X = []\n",
    "\n",
    "for line in lines:\n",
    "    ind_line = []\n",
    "    for word in line:\n",
    "        ind_line.append(word2ind[word])\n",
    "    \n",
    "    if len(ind_line) < maxlen:\n",
    "        ind_line += [0] * (maxlen - len(ind_line))\n",
    "    \n",
    "    X.append(ind_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 91,\n",
       " 5,\n",
       " 371,\n",
       " 500,\n",
       " 197,\n",
       " 500,\n",
       " 4,\n",
       " 500,\n",
       " 500,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7ac9b2fa7466f1f22b6698882ceab5cc9f8d352c9b30a1755d5cef30b1b636f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ai_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
