{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IHB00BRxfsy8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import tqdm\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vvS8ZjFggm7",
        "outputId": "d287d17d-4ed7-4134-a9d4-3fdfe779729a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "608GLteifsy-"
      },
      "source": [
        "# Find Hyperlinks in string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "laJp3mMUfsy_"
      },
      "outputs": [],
      "source": [
        "def findUrl(string):\n",
        "  \n",
        "    # findall() has been used \n",
        "    # with valid conditions for urls in string\n",
        "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "    found = re.search(regex, string)\n",
        "    return found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j62eciqUfsy_"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "Y = list((train_data['Type'] == 'Quality').astype(int))\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B6Nb9M1fsy_"
      },
      "source": [
        "# Data Loader / Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MbbG3D3Gfsy_"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "tokenizer = TweetTokenizer()\n",
        "hashtag = True\n",
        "wordcount = defaultdict(int)\n",
        "vocab_size = 500\n",
        "\n",
        "lines = []\n",
        "maxlen = 0\n",
        "\n",
        "for data in train_data['Tweet']:\n",
        "\n",
        "    line = ['<START>']\n",
        "\n",
        "    tokens = tokenizer.tokenize(data.lower())\n",
        "\n",
        "    for token in tokens:\n",
        "        url = findUrl(token)\n",
        "        if url:\n",
        "            line.append('<URL>')\n",
        "            wordcount['<URL>'] += 1\n",
        "        elif token[0] == '#':\n",
        "            if hashtag:\n",
        "                line.append(token)\n",
        "                wordcount[token] += 1\n",
        "            else:\n",
        "                line.append('<HASH>')\n",
        "                wordcount['<HASH>'] += 1\n",
        "        else:\n",
        "            more_words = word_tokenize(token)\n",
        "            for w in more_words:\n",
        "                line.append(w)\n",
        "                wordcount[w] += 1\n",
        "\n",
        "    line.append('<END>')\n",
        "    maxlen = max(maxlen, len(line))\n",
        "    lines.append(line)\n",
        "\n",
        "wordcount['<START>'] = len(train_data['Tweet'])\n",
        "wordcount['<END>'] = len(train_data['Tweet'])\n",
        "\n",
        "sorted_wordcounts = sorted(wordcount.items(), key = lambda item: item[1], reverse=True)\n",
        "\n",
        "word2ind = {}\n",
        "ind2word = {}\n",
        "\n",
        "ind = 1\n",
        "for k, v in sorted_wordcounts[:vocab_size - 1]:\n",
        "    word2ind[k] = ind\n",
        "    ind2word[ind] = k\n",
        "    ind += 1\n",
        "\n",
        "for k, v in sorted_wordcounts[vocab_size - 1:]:\n",
        "    word2ind[k] = vocab_size\n",
        "    ind2word[vocab_size - 1] = '<UKN>'\n",
        "\n",
        "X = []\n",
        "\n",
        "for line in lines:\n",
        "    ind_line = []\n",
        "    for word in line:\n",
        "        ind_line.append(word2ind[word])\n",
        "    \n",
        "    if len(ind_line) < maxlen:\n",
        "        ind_line += [0] * (maxlen - len(ind_line))\n",
        "    \n",
        "    X.append(ind_line)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3kp-42gfszA"
      },
      "source": [
        "# LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "yswotFkjfszA"
      },
      "outputs": [],
      "source": [
        "class biLSTM(nn.Module):\n",
        "\n",
        "    def __init__ (self, h_dim = 10, e_dim = 10):\n",
        "        super(biLSTM, self).__init__()\n",
        "        \n",
        "        self.h_dim = h_dim\n",
        "        self.e_dim = e_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings= vocab_size + 1, embedding_dim = self.e_dim, padding_idx = 0)\n",
        "        self.pool = torch.nn.AdaptiveAvgPool1d(output_size=1)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size = self.e_dim, \n",
        "                            hidden_size = self.h_dim, \n",
        "                            num_layers = 1,\n",
        "                            batch_first = True,\n",
        "                            bidirectional = True)\n",
        "        \n",
        "        self.drop = nn.Dropout(p = 0.5)\n",
        "        self.linear = nn.Linear(2 * self.h_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, X):\n",
        "\n",
        "        X = self.embedding(X)\n",
        "        X = self.drop(X)\n",
        "        X, _ = self.lstm(X)\n",
        "        X = X.permute(0,2,1)\n",
        "        X = self.pool(X)\n",
        "        X = torch.squeeze(X)\n",
        "        X = self.linear(X)       \n",
        "        X = self.sigmoid(X)\n",
        "\n",
        "        return X\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hm5fl0GUfszB"
      },
      "outputs": [],
      "source": [
        "def shuffle_data(X, Y):\n",
        "    shuffled_X = []\n",
        "    shuffled_Y = []\n",
        "    indices = list(range(len(X)))\n",
        "    random.shuffle(indices)\n",
        "    for i in indices:\n",
        "        shuffled_X.append(X[i])\n",
        "        shuffled_Y.append(Y[i])\n",
        "    return (shuffled_X, shuffled_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftZFdFYlfszB"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "AUp7W1o-fszC"
      },
      "outputs": [],
      "source": [
        "def train(X, Y, lstm):\n",
        "    optimizer = optim.Adam(lstm.parameters(), lr = 0.001)\n",
        "    batchSize = 10\n",
        "    loss_f = nn.BCELoss()\n",
        "    n_epochs = 10\n",
        "\n",
        "    \n",
        "\n",
        "    batchSize = 10\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        lstm.train()\n",
        "        totalLoss = 0.0\n",
        "\n",
        "        shuffled_X, shuffled_Y = shuffle_data(X, Y)\n",
        "        X_train, Y_train = torch.tensor(shuffled_X, device = device), torch.tensor(shuffled_Y, dtype = float, device = device)\n",
        "\n",
        "        for batch in range(len(X_train)//10 + 1):\n",
        "          lstm.zero_grad()\n",
        "          x = X_train[batch*batchSize: (batch + 1)*batchSize]\n",
        "          y = Y_train[batch*batchSize: (batch + 1)*batchSize]\n",
        "          output = lstm(x)\n",
        "          loss = loss_f(output.squeeze().to(dtype = float), y)\n",
        "          totalLoss += loss.item()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "        print(\"total loss is: \", totalLoss)\n",
        "        \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi2Aj8v2fszC",
        "outputId": "1f8555b5-bc1c-473b-ec18-8107405968a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total loss is:  588.1111591619334\n",
            "total loss is:  476.3481225206878\n",
            "total loss is:  434.0615752626056\n",
            "total loss is:  402.93117795570083\n",
            "total loss is:  387.1244311673069\n",
            "total loss is:  365.8011303522309\n",
            "total loss is:  357.699950162021\n",
            "total loss is:  346.04006851951624\n",
            "total loss is:  327.97060857919416\n",
            "total loss is:  317.2938399278921\n"
          ]
        }
      ],
      "source": [
        "lstm = biLSTM(e_dim = 300, h_dim = 50).to(device)\n",
        "train(X[1000:], Y[1000:], lstm)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_X, test_Y):\n",
        "  y_pred = []\n",
        "  input = torch.tensor(test_X, dtype=int, device = device)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  output = model(input).squeeze()\n",
        "  output = (output > 0.5).int()\n",
        "  y_pred = output.tolist()\n",
        "  \n",
        "  print(classification_report(test_Y, y_pred, labels=[1, 0], digits = 4))\n",
        "\n",
        "evaluate(lstm, X[:1000], Y[:1000])\n",
        "\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ieo7qImjDk7o",
        "outputId": "90d2e1c6-13e4-4be2-ac7f-d989c6de018a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8784    0.8099    0.8427       526\n",
            "           0     0.8058    0.8755    0.8392       474\n",
            "\n",
            "    accuracy                         0.8410      1000\n",
            "   macro avg     0.8421    0.8427    0.8410      1000\n",
            "weighted avg     0.8440    0.8410    0.8411      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries for reading data, exploring and plotting\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "%matplotlib inline\n",
        "# library for train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# deep learning libraries for text pre-processing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Modeling \n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, LSTM, Bidirectional"
      ],
      "metadata": {
        "id": "0R6eI_UmBwmv"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Biderectional LSTM Spam detection architecture\n",
        "embeding_dim = 300\n",
        "max_len = 81\n",
        "n_lstm = 50\n",
        "drop_lstm = 0.2\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(vocab_size + 1, embeding_dim, input_length=max_len))\n",
        "model2.add(Bidirectional(LSTM(n_lstm, dropout=drop_lstm, return_sequences=True)))\n",
        "model2.add(GlobalAveragePooling1D())\n",
        "model2.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "AyF-I4rUB1Qz"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "d87rNS2jB94R"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOi7NDDlCHe7",
        "outputId": "7b292adf-1a00-4548-b6ef-8e70126302e1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 81, 300)           150300    \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 81, 100)          140400    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " global_average_pooling1d_3   (None, 100)              0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 290,801\n",
            "Trainable params: 290,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_padded = X[:len(X)//2]\n",
        "train_labels = Y[:len(X)//2]\n",
        "testing_padded = X[len(X)//2:]\n",
        "test_labels = Y[len(X)//2:] "
      ],
      "metadata": {
        "id": "8_YcLBiTCexl"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "num_epochs = 30\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
        "history = model2.fit(training_padded, train_labels, epochs=num_epochs, \n",
        "                    validation_data=(testing_padded, test_labels),callbacks =[early_stop], verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO5WUxNGCIZp",
        "outputId": "b428e3ed-1bf4-48ea-efe2-b58d1440a2a9"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "187/187 - 10s - loss: 0.5995 - accuracy: 0.6671 - val_loss: 0.5003 - val_accuracy: 0.7619 - 10s/epoch - 53ms/step\n",
            "Epoch 2/30\n",
            "187/187 - 3s - loss: 0.5100 - accuracy: 0.7565 - val_loss: 0.4875 - val_accuracy: 0.7904 - 3s/epoch - 15ms/step\n",
            "Epoch 3/30\n",
            "187/187 - 3s - loss: 0.4580 - accuracy: 0.7886 - val_loss: 0.4701 - val_accuracy: 0.7670 - 3s/epoch - 15ms/step\n",
            "Epoch 4/30\n",
            "187/187 - 3s - loss: 0.4253 - accuracy: 0.8041 - val_loss: 0.4571 - val_accuracy: 0.7868 - 3s/epoch - 15ms/step\n",
            "Epoch 5/30\n",
            "187/187 - 3s - loss: 0.4096 - accuracy: 0.8199 - val_loss: 0.4572 - val_accuracy: 0.7955 - 3s/epoch - 15ms/step\n",
            "Epoch 6/30\n",
            "187/187 - 3s - loss: 0.4340 - accuracy: 0.8060 - val_loss: 0.4543 - val_accuracy: 0.7909 - 3s/epoch - 15ms/step\n",
            "Epoch 7/30\n",
            "187/187 - 3s - loss: 0.3895 - accuracy: 0.8250 - val_loss: 0.4212 - val_accuracy: 0.8065 - 3s/epoch - 15ms/step\n",
            "Epoch 8/30\n",
            "187/187 - 3s - loss: 0.3641 - accuracy: 0.8324 - val_loss: 0.4328 - val_accuracy: 0.7976 - 3s/epoch - 15ms/step\n",
            "Epoch 9/30\n",
            "187/187 - 3s - loss: 0.3491 - accuracy: 0.8374 - val_loss: 0.4111 - val_accuracy: 0.8102 - 3s/epoch - 15ms/step\n",
            "Epoch 10/30\n",
            "187/187 - 3s - loss: 0.3211 - accuracy: 0.8559 - val_loss: 0.4078 - val_accuracy: 0.8147 - 3s/epoch - 15ms/step\n",
            "Epoch 11/30\n",
            "187/187 - 3s - loss: 0.3038 - accuracy: 0.8661 - val_loss: 0.4118 - val_accuracy: 0.8157 - 3s/epoch - 15ms/step\n",
            "Epoch 12/30\n",
            "187/187 - 4s - loss: 0.2893 - accuracy: 0.8720 - val_loss: 0.4333 - val_accuracy: 0.8145 - 4s/epoch - 20ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9Pa4VjMFCuxb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "b7ac9b2fa7466f1f22b6698882ceab5cc9f8d352c9b30a1755d5cef30b1b636f"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 ('ai_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "bilstm.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}