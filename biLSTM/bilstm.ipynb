{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Hyperlinks in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findUrl(string):\n",
    "  \n",
    "    # findall() has been used \n",
    "    # with valid conditions for urls in string\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    found = re.search(regex, string)\n",
    "    return found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader / Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../train.csv')\n",
    "tokenizer = TweetTokenizer()\n",
    "hashtag = True\n",
    "wordcount = defaultdict(int)\n",
    "vocab_size = 500\n",
    "\n",
    "lines = []\n",
    "maxlen = 0\n",
    "for data in train_data['Tweet']:\n",
    "\n",
    "    line = ['<START>']\n",
    "\n",
    "    tokens = tokenizer.tokenize(data.lower())\n",
    "\n",
    "    for token in tokens:\n",
    "        url = findUrl(token)\n",
    "        if url:\n",
    "            line.append('<URL>')\n",
    "            wordcount['<URL>'] += 1\n",
    "        elif token[0] == '#':\n",
    "            if hashtag:\n",
    "                line.append(token)\n",
    "                wordcount[token] += 1\n",
    "            else:\n",
    "                line.append('<HASH>')\n",
    "                wordcount['<HASH>'] += 1\n",
    "        else:\n",
    "            more_words = word_tokenize(token)\n",
    "            for w in more_words:\n",
    "                line.append(w)\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    line.append('<END>')\n",
    "    maxlen = max(maxlen, len(line))\n",
    "    lines.append(line)\n",
    "\n",
    "wordcount['<START>'] = len(train_data['Tweet'])\n",
    "wordcount['<END>'] = len(train_data['Tweet'])\n",
    "\n",
    "sorted_wordcounts = sorted(wordcount.items(), key = lambda item: item[1], reverse=True)\n",
    "\n",
    "word2ind = {}\n",
    "ind2word = {}\n",
    "\n",
    "ind = 1\n",
    "for k, v in sorted_wordcounts[:vocab_size - 1]:\n",
    "    word2ind[k] = ind\n",
    "    ind2word[ind] = k\n",
    "    ind += 1\n",
    "\n",
    "for k, v in sorted_wordcounts[vocab_size - 1:]:\n",
    "    word2ind[k] = vocab_size\n",
    "    ind2word[vocab_size - 1] = '<UKN>'\n",
    "\n",
    "X = []\n",
    "\n",
    "for line in lines:\n",
    "    ind_line = []\n",
    "    for word in line:\n",
    "        ind_line.append(word2ind[word])\n",
    "    \n",
    "    if len(ind_line) < maxlen:\n",
    "        ind_line += [0] * (maxlen - len(ind_line))\n",
    "    \n",
    "    X.append(ind_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'it', \"'s\", 'the', 'everything', 'else', 'that', \"'s\", 'complicated', '.', '#pesummit', '#pxpic', '<URL>', '<END>']\n",
      "[1, 26, 21, 5, 354, 500, 29, 21, 500, 4, 500, 500, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(lines[0])\n",
    "print(X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7ac9b2fa7466f1f22b6698882ceab5cc9f8d352c9b30a1755d5cef30b1b636f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ai_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
