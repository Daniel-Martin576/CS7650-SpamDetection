{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing & Importing Dependencies","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:42:56.166006Z","iopub.execute_input":"2022-05-04T07:42:56.166311Z","iopub.status.idle":"2022-05-04T07:42:56.192239Z","shell.execute_reply.started":"2022-05-04T07:42:56.166234Z","shell.execute_reply":"2022-05-04T07:42:56.191603Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install transformers --quiet","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:42:56.225964Z","iopub.execute_input":"2022-05-04T07:42:56.226258Z","iopub.status.idle":"2022-05-04T07:43:06.601262Z","shell.execute_reply.started":"2022-05-04T07:42:56.226229Z","shell.execute_reply":"2022-05-04T07:43:06.600461Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# SciKit Learn\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix\n\n# Transformers\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\nfrom transformers.models.bert.modeling_bert import BertEmbeddings\n\n# Others\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:06.603397Z","iopub.execute_input":"2022-05-04T07:43:06.603685Z","iopub.status.idle":"2022-05-04T07:43:13.168905Z","shell.execute_reply.started":"2022-05-04T07:43:06.603648Z","shell.execute_reply":"2022-05-04T07:43:13.168182Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Global Config\nPRE_TRAINED_MODEL_NAME = 'prajjwal1/bert-tiny'                     # Pre-trained model\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)  # Tokenizer\n\nEPOCHS=5              # Number of epochs\nMAX_LEN=512            # Maximum Length\nBATCH_SIZE=16          # Batch Size\ndevice = 'cuda'        # GPU ","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:13.170082Z","iopub.execute_input":"2022-05-04T07:43:13.170346Z","iopub.status.idle":"2022-05-04T07:43:16.910473Z","shell.execute_reply.started":"2022-05-04T07:43:13.170300Z","shell.execute_reply":"2022-05-04T07:43:16.909783Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Reading & Preprocessing Data","metadata":{}},{"cell_type":"code","source":"### SOURCE, PATH, MSG, SPAM\ndata_configs = [\n    (\"Youtube\", \"../input/cs7650-dataset/Dataset/Youtube/youtube_train.csv\", '''df[\"CONTENT\"]''', '''df[\"CLASS\"]'''),\n    (\"SMS\", \"https://raw.githubusercontent.com/animesharma3/SPAM-SMS-Detection/master/spam_sms_collection.csv\", '''df[\"msg\"]''', '''df[\"spam\"]'''),\n    (\"Email\", \"../input/cs7650-dataset/Dataset/Email/email_spam.csv\", '''df[\"text\"]''', '''(1 * (df[\"label_num\"] == 0))'''),\n    (\"Twitter\", \"../input/cs7650-dataset/Dataset/Twitter/train.csv\", '''df[\"Tweet\"]''', '''(1 * (df[\"Type\"] == \"Quality\"))''')\n]\n\ndata_frames = {}\n\nfor config in data_configs:\n    src, path, msg, spam = config\n    df = pd.read_csv(path)\n    df['msg'] = eval(msg)\n    df['spam'] = eval(spam)\n    df = df[['msg', 'spam']]\n    data_frames[src] = df","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:16.912775Z","iopub.execute_input":"2022-05-04T07:43:16.913118Z","iopub.status.idle":"2022-05-04T07:43:17.444348Z","shell.execute_reply.started":"2022-05-04T07:43:16.913075Z","shell.execute_reply":"2022-05-04T07:43:17.443618Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Youtube Data\ndata_frames[\"Youtube\"].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:17.445493Z","iopub.execute_input":"2022-05-04T07:43:17.445764Z","iopub.status.idle":"2022-05-04T07:43:17.461204Z","shell.execute_reply.started":"2022-05-04T07:43:17.445729Z","shell.execute_reply":"2022-05-04T07:43:17.459762Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Twitter Data\ndata_frames[\"Twitter\"].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:17.462491Z","iopub.execute_input":"2022-05-04T07:43:17.462778Z","iopub.status.idle":"2022-05-04T07:43:17.470484Z","shell.execute_reply.started":"2022-05-04T07:43:17.462742Z","shell.execute_reply":"2022-05-04T07:43:17.469619Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# SMS Data\ndata_frames[\"SMS\"].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:17.471746Z","iopub.execute_input":"2022-05-04T07:43:17.472694Z","iopub.status.idle":"2022-05-04T07:43:17.483597Z","shell.execute_reply.started":"2022-05-04T07:43:17.472657Z","shell.execute_reply":"2022-05-04T07:43:17.482709Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Email Data\ndata_frames[\"Email\"].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:17.485426Z","iopub.execute_input":"2022-05-04T07:43:17.485906Z","iopub.status.idle":"2022-05-04T07:43:17.494856Z","shell.execute_reply.started":"2022-05-04T07:43:17.485870Z","shell.execute_reply":"2022-05-04T07:43:17.493972Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class SpamDataset(Dataset):\n    def __init__(self, spam, msgs, tokenizer, max_len):\n        self.msgs = msgs\n        self.spam = spam\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.msgs)\n\n    def __getitem__(self, i):\n        msg = str(self.msgs[i])\n        spam = self.spam[i]\n\n        encoding = self.tokenizer.encode_plus(\n            msg, \n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'msg': msg,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'spam': torch.tensor(spam, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:17.496161Z","iopub.execute_input":"2022-05-04T07:43:17.496958Z","iopub.status.idle":"2022-05-04T07:43:17.505836Z","shell.execute_reply.started":"2022-05-04T07:43:17.496923Z","shell.execute_reply":"2022-05-04T07:43:17.505075Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = SpamDataset(\n        spam=df['spam'].to_numpy(),\n        msgs=df['msg'].to_numpy(),\n        tokenizer=tokenizer,\n        max_len=max_len\n    )\n\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        num_workers=4\n    )","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:17.508815Z","iopub.execute_input":"2022-05-04T07:43:17.509010Z","iopub.status.idle":"2022-05-04T07:43:17.517424Z","shell.execute_reply.started":"2022-05-04T07:43:17.508988Z","shell.execute_reply":"2022-05-04T07:43:17.516727Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Returns train, test, validation data frames\ndef getDataFrames(source):\n    df = data_frames[source]\n    df_train, df_test = tts(\n        df,\n        test_size=0.2,\n        random_state=42,\n        shuffle=True,\n    )\n    df_val, _ = tts(\n        df,\n        test_size=0.5,\n        random_state=42,\n        shuffle=True,\n    )\n    return df_train, df_test, df_val","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:17.518908Z","iopub.execute_input":"2022-05-04T07:43:17.519413Z","iopub.status.idle":"2022-05-04T07:43:17.526335Z","shell.execute_reply.started":"2022-05-04T07:43:17.519377Z","shell.execute_reply":"2022-05-04T07:43:17.525595Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Test getDataFrames with Twitter data\n\ntwitter_train, twitter_test, twitter_val = getDataFrames(\"Twitter\")\nprint(\"twitter_train\", twitter_train.shape)\nprint(\"twitter_test\", twitter_test.shape)\nprint(\"twitter_val\", twitter_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:17.527208Z","iopub.execute_input":"2022-05-04T07:43:17.527399Z","iopub.status.idle":"2022-05-04T07:43:17.541866Z","shell.execute_reply.started":"2022-05-04T07:43:17.527359Z","shell.execute_reply":"2022-05-04T07:43:17.541182Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"### 1. CNN","metadata":{}},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, NUM_CLASSES = 2, DIM_EMB = MAX_LEN):\n        super(CNN, self).__init__()\n        self.Embedding = self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME).get_input_embeddings()\n        self.conv1d_list = nn.ModuleList([\n            nn.Conv1d(in_channels=DIM_EMB,\n                      out_channels=2,\n                      kernel_size=ks)\n            for ks in range(2, 5)\n        ])\n        self.ReLU = nn.ReLU()\n        self.MaxPool = nn.MaxPool1d\n        self.Dropout = nn.Dropout()\n        self.Linear = nn.Linear(6, NUM_CLASSES)\n        self.LogSoftmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input_ids, attention_mask):\n        E = self.Embedding(input_ids)\n        R = [self.ReLU(conv1d(E)) for conv1d in self.conv1d_list]\n        M = [self.MaxPool(kernel_size=r.shape[2])(r) for r in R]\n        C = torch.cat([m.squeeze(dim=2) for m in M], dim = 1)\n        L = self.Linear(C)\n        X = self.LogSoftmax(L)\n        return X","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:17.543222Z","iopub.execute_input":"2022-05-04T07:43:17.543527Z","iopub.status.idle":"2022-05-04T07:43:17.552482Z","shell.execute_reply.started":"2022-05-04T07:43:17.543495Z","shell.execute_reply":"2022-05-04T07:43:17.551743Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### 2. BERT","metadata":{}},{"cell_type":"code","source":"class BERT(nn.Module):\n    def __init__(self, n_classes):\n        super(BERT, self).__init__()\n        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        pooled_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )[1]\n        output = self.drop(pooled_output)\n        return self.out(output)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:17.554057Z","iopub.execute_input":"2022-05-04T07:43:17.554519Z","iopub.status.idle":"2022-05-04T07:43:17.564211Z","shell.execute_reply.started":"2022-05-04T07:43:17.554487Z","shell.execute_reply":"2022-05-04T07:43:17.563350Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### 3. biLSTM","metadata":{}},{"cell_type":"code","source":"class biLSTM(nn.Module):\n    def __init__(self, h_dim=100, max_len=MAX_LEN, emd_dim=100, lstm_layers=1, dropout_rate=0.2):\n        super(biLSTM, self).__init__()\n        self.embeddings = self.bert = BertModel.from_pretrained(\n            PRE_TRAINED_MODEL_NAME).get_input_embeddings()\n#         print(self.embeddings)\n\n        self.text_len = max_len\n        self.lstm_layers = lstm_layers\n        self.h_dim = h_dim\n        \n        # self.embedding = nn.Embedding(max_len, max_len, padding_idx=0)\n        self.lstm = nn.LSTM(input_size=128,\n                            hidden_size=h_dim,\n                            num_layers=lstm_layers,\n                            batch_first=True,\n                            bidirectional=True)\n        self.fc = nn.Linear(2 * lstm_layers * h_dim, 2)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_ids, attention_mask):\n        N, L = input_ids.shape\n        embedding_output = self.embeddings(input_ids)\n        out, (h, c) = self.lstm(embedding_output)\n        h = h.permute(1, 0, 2).resize(N, 2 * self.lstm_layers * self.h_dim)\n\n        return torch.sigmoid(self.fc(h))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:17.566021Z","iopub.execute_input":"2022-05-04T07:43:17.567975Z","iopub.status.idle":"2022-05-04T07:43:17.577224Z","shell.execute_reply.started":"2022-05-04T07:43:17.567939Z","shell.execute_reply":"2022-05-04T07:43:17.576479Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# List of Models\nmodels = {\n    \"CNN\": nn.ModuleDict({\n        \"model\": CNN().to(device),\n        \"loss_fn\": nn.NLLLoss().to(device)\n    }),\n    \"BERT\": nn.ModuleDict({\n        \"model\": BERT(n_classes=2).to(device),\n        \"loss_fn\": nn.CrossEntropyLoss().to(device)\n    }),\n    \"biLSTM\": nn.ModuleDict({\n        \"model\": biLSTM(h_dim=100, lstm_layers=2).to(device),\n        \"loss_fn\": nn.CrossEntropyLoss().to(device)\n    }),\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:17.578691Z","iopub.execute_input":"2022-05-04T07:43:17.579287Z","iopub.status.idle":"2022-05-04T07:43:27.224819Z","shell.execute_reply.started":"2022-05-04T07:43:17.579248Z","shell.execute_reply":"2022-05-04T07:43:27.224077Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Training Function","metadata":{}},{"cell_type":"code","source":"def train(\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    device,\n    data_loader,\n    n_examples\n):\n    model = model.train() # Setting Model in training mode\n\n    losses = []\n    correct_predictions = 0\n\n    for d in data_loader:\n        input_ids = d['input_ids'].to(device)  # [16, 512]\n        attention_mask = d['attention_mask'].to(device)  # [16, 512]\n        targets = d['spam'].to(device)  # [16]\n\n        # Forward Propogation\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        ) # [16, 3]\n\n        # Calculating Loss\n        loss = loss_fn(outputs, targets)\n\n        _, preds = torch.max(outputs, dim=1)\n\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        \n        # Backward Propogation\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Clipping Gradient (Exploding Gradient Problem)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad() # Resetting gradients\n\n    train_acc = correct_predictions.double() / n_examples\n    train_loss = np.mean(losses)\n    \n    return train_acc, train_loss","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:27.226172Z","iopub.execute_input":"2022-05-04T07:43:27.226423Z","iopub.status.idle":"2022-05-04T07:43:27.235463Z","shell.execute_reply.started":"2022-05-04T07:43:27.226386Z","shell.execute_reply":"2022-05-04T07:43:27.234806Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Function","metadata":{}},{"cell_type":"code","source":"def evaluate_model(\n    model,\n    loss_fn,\n    device,\n    data_loader,\n    n_examples   \n):\n    model = model.eval() # Setting Model in evaluation mode\n\n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].to(device)  # [16, 512]\n            attention_mask = d['attention_mask'].to(device)  # [16, 512]\n            targets = d['spam'].to(device)  # [16]\n\n            # Forward Propogation\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            ) # [16, 3]\n\n            # Calculating Loss\n            loss = loss_fn(outputs, targets)\n\n            _, preds = torch.max(outputs, dim=1)\n\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n        \n    train_acc = correct_predictions.double() / n_examples\n    train_loss = np.mean(losses)\n\n    return train_acc, train_loss","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:27.236592Z","iopub.execute_input":"2022-05-04T07:43:27.237142Z","iopub.status.idle":"2022-05-04T07:43:27.248010Z","shell.execute_reply.started":"2022-05-04T07:43:27.237107Z","shell.execute_reply":"2022-05-04T07:43:27.247290Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def run_epochs(p):    \n    history = defaultdict(list)\n    best_accuracy = 0\n\n    for epoch in range(EPOCHS):\n        print(f'Epoch {epoch + 1}/{EPOCHS}')\n        print('-' * 10)\n        \n        train_acc, train_loss = train(\n            p[\"model\"],\n            p[\"loss_fn\"],\n            p[\"optimizer\"],\n            p[\"scheduler\"],\n            p[\"device\"],\n            p[\"train_data_loader\"],\n            len(p[\"df_train\"])\n        )\n\n        print(f'Train loss {train_loss} accuracy {train_acc}')\n\n        val_acc, val_loss = evaluate_model(\n            p[\"model\"],\n            p[\"loss_fn\"],\n            p[\"device\"],\n            p[\"val_data_loader\"],\n            len(p[\"df_val\"])\n        )\n\n        print(f'Validation loss {val_loss} accuracy {val_acc}')\n        print()\n\n        history['train_acc'].append(train_acc)\n        history['train_loss'].append(train_loss)\n        history['val_acc'].append(val_acc)\n        history['val_loss'].append(val_loss)\n\n        if val_acc > best_accuracy:\n            torch.save(model.state_dict(), f'M_{p[\"modelName\"]}_D_{p[\"data_source\"]}_best_model_state.bin')\n            best_accuracy = val_acc","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:27.250035Z","iopub.execute_input":"2022-05-04T07:43:27.250226Z","iopub.status.idle":"2022-05-04T07:43:27.261065Z","shell.execute_reply.started":"2022-05-04T07:43:27.250204Z","shell.execute_reply":"2022-05-04T07:43:27.260258Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Run Models","metadata":{}},{"cell_type":"code","source":"%%time\n\nfor modelName in models.keys():\n    print(f\"________ MODEL: {modelName} ________\")\n    \n    ### Model Unpacking\n    m = models[modelName]\n    model = m[\"model\"]\n    loss_fn = m[\"loss_fn\"]\n    optimizer = AdamW(model.parameters(), lr=2e-3, correct_bias=False)\n\n    for data_source in data_frames.keys():\n        print(f\"________ {modelName} running on {data_source.upper()} ________\")\n        \n        \n        # Data Unpacking\n        df_train, df_test, df_val = getDataFrames(data_source)\n        train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n        test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n        val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n        \n#         print(\"opt\", type(optimizer))\n        # Total Steps & Scheduler\n        total_steps = len(train_data_loader) * EPOCHS\n        scheduler = get_linear_schedule_with_warmup(\n          optimizer,\n          num_warmup_steps=0,\n          num_training_steps=total_steps\n        )\n        \n        # Parameters\n        params = {\n            \"modelName\": modelName,\n            \"data_source\": data_source,\n            \"model\": model,\n            \"loss_fn\": loss_fn,\n            \"optimizer\": optimizer,\n            \"scheduler\": scheduler,\n            \"device\": device,\n            \"train_data_loader\": train_data_loader,\n            \"test_data_loader\": test_data_loader,\n            \"val_data_loader\": val_data_loader,\n            \"df_train\" :df_train,\n            \"df_val\": df_val,\n            \"df_test\": df_test\n        }\n\n        run_epochs(params)\n        \n        test_acc, _ = evaluate_model(\n            model,\n            loss_fn,\n            device,\n            test_data_loader,\n            len(df_test)\n        )\n        print(f\"TEST ACCURACY: \\t {test_acc.item()}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:43:27.262400Z","iopub.execute_input":"2022-05-04T07:43:27.262958Z","iopub.status.idle":"2022-05-04T07:49:24.416212Z","shell.execute_reply.started":"2022-05-04T07:43:27.262922Z","shell.execute_reply":"2022-05-04T07:49:24.415463Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}