{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "spam-detector-cs-7650.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing & Importing Dependencies"
      ],
      "metadata": {
        "id": "PSR9nMzPOaWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:42:56.166006Z",
          "iopub.execute_input": "2022-05-04T07:42:56.166311Z",
          "iopub.status.idle": "2022-05-04T07:42:56.192239Z",
          "shell.execute_reply.started": "2022-05-04T07:42:56.166234Z",
          "shell.execute_reply": "2022-05-04T07:42:56.191603Z"
        },
        "trusted": true,
        "id": "dBAcDZV2OaWT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:42:56.225964Z",
          "iopub.execute_input": "2022-05-04T07:42:56.226258Z",
          "iopub.status.idle": "2022-05-04T07:43:06.601262Z",
          "shell.execute_reply.started": "2022-05-04T07:42:56.226229Z",
          "shell.execute_reply": "2022-05-04T07:43:06.600461Z"
        },
        "trusted": true,
        "id": "G7e5X0pWOaWT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# SciKit Learn\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix\n",
        "\n",
        "# Transformers\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers.models.bert.modeling_bert import BertEmbeddings\n",
        "\n",
        "# Others\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:06.603397Z",
          "iopub.execute_input": "2022-05-04T07:43:06.603685Z",
          "iopub.status.idle": "2022-05-04T07:43:13.168905Z",
          "shell.execute_reply.started": "2022-05-04T07:43:06.603648Z",
          "shell.execute_reply": "2022-05-04T07:43:13.168182Z"
        },
        "trusted": true,
        "id": "iNB6wQliOaWU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Config\n",
        "PRE_TRAINED_MODEL_NAME = 'prajjwal1/bert-tiny'                     # Pre-trained model\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)  # Tokenizer\n",
        "\n",
        "EPOCHS=10              # Number of epochs\n",
        "MAX_LEN=512            # Maximum Length\n",
        "BATCH_SIZE=16          # Batch Size\n",
        "device = 'cuda'        # GPU "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:13.170082Z",
          "iopub.execute_input": "2022-05-04T07:43:13.170346Z",
          "iopub.status.idle": "2022-05-04T07:43:16.910473Z",
          "shell.execute_reply.started": "2022-05-04T07:43:13.170300Z",
          "shell.execute_reply": "2022-05-04T07:43:16.909783Z"
        },
        "trusted": true,
        "id": "vjmpk6zzOaWU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading & Preprocessing Data"
      ],
      "metadata": {
        "id": "eaj0GY7ROaWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### SOURCE, PATH, MSG, SPAM\n",
        "# data_configs = [\n",
        "#     (\"Youtube\", \"../input/cs7650-dataset/Dataset/Youtube/youtube_train.csv\", '''df[\"CONTENT\"]''', '''df[\"CLASS\"]'''),\n",
        "#     (\"SMS\", \"https://raw.githubusercontent.com/animesharma3/SPAM-SMS-Detection/master/spam_sms_collection.csv\", '''df[\"msg\"]''', '''df[\"spam\"]'''),\n",
        "#     (\"Email\", \"../input/cs7650-dataset/Dataset/Email/email_spam.csv\", '''df[\"text\"]''', '''(1 * (df[\"label_num\"] == 0))'''),\n",
        "#     (\"Twitter\", \"../input/cs7650-dataset/Dataset/Twitter/train.csv\", '''df[\"Tweet\"]''', '''(1 * (df[\"Type\"] == \"Quality\"))''')\n",
        "# ]\n",
        "\n",
        "data_configs = [\n",
        "    (\"Youtube\", \"youtube_train.csv\", '''df[\"CONTENT\"]''', '''df[\"CLASS\"]'''),\n",
        "    (\"SMS\", \"https://raw.githubusercontent.com/animesharma3/SPAM-SMS-Detection/master/spam_sms_collection.csv\", '''df[\"msg\"]''', '''df[\"spam\"]'''),\n",
        "    (\"Email\", \"email_spam.csv\", '''df[\"text\"]''', '''(1 * (df[\"label_num\"] == 1))'''),\n",
        "    (\"Twitter\", \"train.csv\", '''df[\"Tweet\"]''', '''(1 * (df[\"Type\"] == \"Spam\"))''')\n",
        "]\n",
        "\n",
        "data_frames = {}\n",
        "\n",
        "for config in data_configs:\n",
        "    src, path, msg, spam = config\n",
        "    df = pd.read_csv(path)\n",
        "    df['msg'] = eval(msg)\n",
        "    df['spam'] = eval(spam)\n",
        "    df = df[['msg', 'spam']]\n",
        "    data_frames[src] = df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:16.912775Z",
          "iopub.execute_input": "2022-05-04T07:43:16.913118Z",
          "iopub.status.idle": "2022-05-04T07:43:17.444348Z",
          "shell.execute_reply.started": "2022-05-04T07:43:16.913075Z",
          "shell.execute_reply": "2022-05-04T07:43:17.443618Z"
        },
        "trusted": true,
        "id": "EpIzQMykOaWV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, spam, msgs, tokenizer, max_len):\n",
        "        self.msgs = msgs\n",
        "        self.spam = spam\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.msgs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        msg = str(self.msgs[i])\n",
        "        spam = self.spam[i]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            msg, \n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'msg': msg,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'spam': torch.tensor(spam, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:17.496161Z",
          "iopub.execute_input": "2022-05-04T07:43:17.496958Z",
          "iopub.status.idle": "2022-05-04T07:43:17.505836Z",
          "shell.execute_reply.started": "2022-05-04T07:43:17.496923Z",
          "shell.execute_reply": "2022-05-04T07:43:17.505075Z"
        },
        "trusted": true,
        "id": "gHNu7hEjOaWW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "    ds = SpamDataset(\n",
        "        spam=df['spam'].to_numpy(),\n",
        "        msgs=df['msg'].to_numpy(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=4\n",
        "    )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:17.508815Z",
          "iopub.execute_input": "2022-05-04T07:43:17.509010Z",
          "iopub.status.idle": "2022-05-04T07:43:17.517424Z",
          "shell.execute_reply.started": "2022-05-04T07:43:17.508988Z",
          "shell.execute_reply": "2022-05-04T07:43:17.516727Z"
        },
        "trusted": true,
        "id": "PkbK0yKcOaWX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns train, test, validation data frames\n",
        "def getDataFrames(source):\n",
        "    df = data_frames[source]\n",
        "    df_train, df_val = tts(\n",
        "        df,\n",
        "        test_size=0.2,\n",
        "        shuffle=True,\n",
        "    )\n",
        "    _, df_test = tts(\n",
        "        df_val,\n",
        "        test_size=0.5,\n",
        "        shuffle=True,\n",
        "    )\n",
        "    return df_train, df_test, df_val"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:17.518908Z",
          "iopub.execute_input": "2022-05-04T07:43:17.519413Z",
          "iopub.status.idle": "2022-05-04T07:43:17.526335Z",
          "shell.execute_reply.started": "2022-05-04T07:43:17.519377Z",
          "shell.execute_reply": "2022-05-04T07:43:17.525595Z"
        },
        "trusted": true,
        "id": "3KcuQ2jUOaWX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "K2d3sDA3OaWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. CNN"
      ],
      "metadata": {
        "id": "XiODSouEOaWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, NUM_CLASSES = 2, DIM_EMB = MAX_LEN):\n",
        "        super(CNN, self).__init__()\n",
        "        self.Embedding = self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME).get_input_embeddings()\n",
        "        self.conv1d_list = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=DIM_EMB,\n",
        "                      out_channels=2,\n",
        "                      kernel_size=ks)\n",
        "            for ks in range(2, 5)\n",
        "        ])\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.MaxPool = nn.MaxPool1d\n",
        "        self.Dropout = nn.Dropout()\n",
        "        self.Linear = nn.Linear(6, NUM_CLASSES)\n",
        "        self.LogSoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        E = self.Embedding(input_ids)\n",
        "        R = [self.ReLU(conv1d(E)) for conv1d in self.conv1d_list]\n",
        "        M = [self.MaxPool(kernel_size=r.shape[2])(r) for r in R]\n",
        "        C = torch.cat([m.squeeze(dim=2) for m in M], dim = 1)\n",
        "        L = self.Linear(C)\n",
        "        X = self.LogSoftmax(L)\n",
        "        return X"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:17.543222Z",
          "iopub.execute_input": "2022-05-04T07:43:17.543527Z",
          "iopub.status.idle": "2022-05-04T07:43:17.552482Z",
          "shell.execute_reply.started": "2022-05-04T07:43:17.543495Z",
          "shell.execute_reply": "2022-05-04T07:43:17.551743Z"
        },
        "trusted": true,
        "id": "exItBao7OaWX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. BERT"
      ],
      "metadata": {
        "id": "128Hc-R0OaWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BERT, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        pooled_output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )[1]\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:17.554057Z",
          "iopub.execute_input": "2022-05-04T07:43:17.554519Z",
          "iopub.status.idle": "2022-05-04T07:43:17.564211Z",
          "shell.execute_reply.started": "2022-05-04T07:43:17.554487Z",
          "shell.execute_reply": "2022-05-04T07:43:17.563350Z"
        },
        "trusted": true,
        "id": "P9gwM_y3OaWY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. biLSTM"
      ],
      "metadata": {
        "id": "pK8QvrkNOaWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class biLSTM(nn.Module):\n",
        "    def __init__(self, h_dim=100, max_len=MAX_LEN, emd_dim=100, lstm_layers=1, dropout_rate=0.2):\n",
        "        super(biLSTM, self).__init__()\n",
        "        self.embeddings = self.bert = BertModel.from_pretrained(\n",
        "            PRE_TRAINED_MODEL_NAME).get_input_embeddings()\n",
        "#         print(self.embeddings)\n",
        "\n",
        "        self.text_len = max_len\n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.h_dim = h_dim\n",
        "        \n",
        "        # self.embedding = nn.Embedding(max_len, max_len, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size=128,\n",
        "                            hidden_size=h_dim,\n",
        "                            num_layers=lstm_layers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        self.fc = nn.Linear(2 * lstm_layers * h_dim, 2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        N, L = input_ids.shape\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "        out, (h, c) = self.lstm(embedding_output)\n",
        "        h = h.permute(1, 0, 2).resize(N, 2 * self.lstm_layers * self.h_dim)\n",
        "\n",
        "        return torch.sigmoid(self.fc(h))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:17.566021Z",
          "iopub.execute_input": "2022-05-04T07:43:17.567975Z",
          "iopub.status.idle": "2022-05-04T07:43:17.577224Z",
          "shell.execute_reply.started": "2022-05-04T07:43:17.567939Z",
          "shell.execute_reply": "2022-05-04T07:43:17.576479Z"
        },
        "trusted": true,
        "id": "agKl9nEAOaWY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of Models\n",
        "models = {\n",
        "    # \"CNN\": nn.ModuleDict({\n",
        "    #     \"model\": CNN().to(device),\n",
        "    #     \"loss_fn\": nn.NLLLoss().to(device)\n",
        "    # }),\n",
        "    # \"BERT\": nn.ModuleDict({\n",
        "    #     \"model\": BERT(n_classes=2).to(device),\n",
        "    #     \"loss_fn\": nn.CrossEntropyLoss().to(device)\n",
        "    # }),\n",
        "    \"biLSTM\": nn.ModuleDict({\n",
        "        \"model\": biLSTM(h_dim=100, lstm_layers=2).to(device),\n",
        "        \"loss_fn\": nn.CrossEntropyLoss().to(device)\n",
        "    }),\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:17.578691Z",
          "iopub.execute_input": "2022-05-04T07:43:17.579287Z",
          "iopub.status.idle": "2022-05-04T07:43:27.224819Z",
          "shell.execute_reply.started": "2022-05-04T07:43:17.579248Z",
          "shell.execute_reply": "2022-05-04T07:43:27.224077Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_1Rorh3OaWY",
        "outputId": "d5d43bc6-da94-4610-b45a-1d31f8352a49"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Function"
      ],
      "metadata": {
        "id": "QNBMDSrFOaWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    model,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    data_loader,\n",
        "    n_examples\n",
        "):\n",
        "    model = model.train() # Setting Model in training mode\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d['input_ids'].to(device)  # [16, 512]\n",
        "        attention_mask = d['attention_mask'].to(device)  # [16, 512]\n",
        "        targets = d['spam'].to(device)  # [16]\n",
        "\n",
        "        # Forward Propogation\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        ) # [16, 3]\n",
        "\n",
        "        # Calculating Loss\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        # Backward Propogation\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Clipping Gradient (Exploding Gradient Problem)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad() # Resetting gradients\n",
        "\n",
        "    train_acc = correct_predictions.double() / n_examples\n",
        "    train_loss = np.mean(losses)\n",
        "    \n",
        "    return train_acc, train_loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:27.226172Z",
          "iopub.execute_input": "2022-05-04T07:43:27.226423Z",
          "iopub.status.idle": "2022-05-04T07:43:27.235463Z",
          "shell.execute_reply.started": "2022-05-04T07:43:27.226386Z",
          "shell.execute_reply": "2022-05-04T07:43:27.234806Z"
        },
        "trusted": true,
        "id": "hhw1DdvDOaWY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Function"
      ],
      "metadata": {
        "id": "DVZdet7rOaWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(\n",
        "    model,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    data_loader,\n",
        "    n_examples   \n",
        "):\n",
        "    model = model.eval() # Setting Model in evaluation mode\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    \n",
        "    \n",
        "    all_preds, all_targets = None, None\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d['input_ids'].to(device)  # [16, 512]\n",
        "            attention_mask = d['attention_mask'].to(device)  # [16, 512]\n",
        "            targets = d['spam'].to(device)  # [16]\n",
        "\n",
        "            # Forward Propogation\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            ) # [16, 3]\n",
        "\n",
        "            # Calculating Loss\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == targets)\n",
        "            losses.append(loss.item())\n",
        "            if all_preds is None:\n",
        "                all_preds = preds\n",
        "                all_targets = targets\n",
        "            else:\n",
        "                all_preds = torch.cat((all_preds, preds))\n",
        "                all_targets = torch.cat((all_targets, targets))\n",
        "        \n",
        "    f1 = f1_score(all_targets.cpu(), all_preds.cpu())\n",
        "    train_acc = correct_predictions.double() / n_examples\n",
        "    train_loss = np.mean(losses)\n",
        "\n",
        "    return train_acc, train_loss, f1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:27.236592Z",
          "iopub.execute_input": "2022-05-04T07:43:27.237142Z",
          "iopub.status.idle": "2022-05-04T07:43:27.248010Z",
          "shell.execute_reply.started": "2022-05-04T07:43:27.237107Z",
          "shell.execute_reply": "2022-05-04T07:43:27.247290Z"
        },
        "trusted": true,
        "id": "KykHkYd9OaWY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epochs(p):    \n",
        "    history = defaultdict(list)\n",
        "    best_accuracy = 0\n",
        "    best_f1 = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "        print('-' * 10)\n",
        "        \n",
        "        train_acc, train_loss = train(\n",
        "            p[\"model\"],\n",
        "            p[\"loss_fn\"],\n",
        "            p[\"optimizer\"],\n",
        "            p[\"scheduler\"],\n",
        "            p[\"device\"],\n",
        "            p[\"train_data_loader\"],\n",
        "            len(p[\"df_train\"])\n",
        "        )\n",
        "\n",
        "        print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "        total_val_acc = 0\n",
        "        total_val_f1 = 0\n",
        "\n",
        "        for k, v in p['val_data_loader'].items():\n",
        "          print(k)\n",
        "          val_acc, val_loss, val_f1 = evaluate_model(\n",
        "              p[\"model\"],\n",
        "              p[\"loss_fn\"],\n",
        "              p[\"device\"],\n",
        "              v,\n",
        "              len(p['df_vals'][k])\n",
        "          )\n",
        "          total_val_acc += val_acc\n",
        "          total_val_f1 += val_f1\n",
        "\n",
        "          print(f'Validation loss {val_loss} accuracy {val_acc} f1 {val_f1}')\n",
        "          print()\n",
        "        avg_val_acc = total_val_acc / len(p['val_data_loader'])\n",
        "        avg_val_f1 = total_val_f1 / len(p['val_data_loader'])\n",
        "\n",
        "        if avg_val_f1 > best_f1:\n",
        "            torch.save(p['model'], f\"{p['modelName']}_{''.join(p['run_config'])}.pt\")\n",
        "            best_f1 = avg_val_f1\n",
        "            best_accuracy = avg_val_acc\n",
        "    print(f\"best f1: {avg_val_f1}, accuracy: {best_accuracy}\")\n",
        "\n",
        "    if len(p['run_config']) == 3:\n",
        "      model = torch.load(f\"{p['modelName']}_{''.join(p['run_config'])}.pt\")\n",
        "      for data_source in data_frames.keys():\n",
        "        if data_source.lower()[0] not in r_config:\n",
        "          df_train, df_test, df_val = getDataFrames(data_source)\n",
        "          test_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "          test_acc, _, test_f1 = evaluate_model(\n",
        "              model,\n",
        "              loss_fn,\n",
        "              device,\n",
        "              test_data_loader,\n",
        "              len(df_val)\n",
        "          )\n",
        "          print(f\"test f1 score: {test_f1}\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-04T07:43:27.250035Z",
          "iopub.execute_input": "2022-05-04T07:43:27.250226Z",
          "iopub.status.idle": "2022-05-04T07:43:27.261065Z",
          "shell.execute_reply.started": "2022-05-04T07:43:27.250204Z",
          "shell.execute_reply": "2022-05-04T07:43:27.260258Z"
        },
        "trusted": true,
        "id": "AEwjqhb3OaWY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Models"
      ],
      "metadata": {
        "id": "KuEK2p1JOaWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_configs = [('t'),('e','s','t','y'),('e','s','y'),('e'), ('s'),('y'), ('e','s','t'), ('e','t','y'),('s','t','y')]\n",
        "\n",
        "\n",
        "for r_config in run_configs:\n",
        "  test_dls = {} \n",
        "  val_dls = {}\n",
        "\n",
        "  df_vals = {}\n",
        "  df_tests = {}\n",
        "  df_combined = pd.DataFrame([], columns=['msg', 'spam'])\n",
        "\n",
        "  for data_source in data_frames.keys():\n",
        "    if data_source.lower()[0] in r_config:\n",
        "      df_train, df_test, df_val = getDataFrames(data_source)\n",
        "      df_combined = pd.concat([df_combined, df_train])\n",
        "      test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "      test_dls[data_source] = test_data_loader\n",
        "      df_tests[data_source] = df_test\n",
        "\n",
        "      val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "      val_dls[data_source] = val_data_loader\n",
        "      df_vals[data_source] = df_val\n",
        "\n",
        "  df_combined['spam'] = df_combined['spam'].astype(int)\n",
        "  df_combined = df_combined.reset_index(drop=True)\n",
        "\n",
        "  df_train = df_combined\n",
        "  train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "  models = {\n",
        "    # \"CNN\": nn.ModuleDict({\n",
        "    #     \"model\": CNN().to(device),\n",
        "    #     \"loss_fn\": nn.NLLLoss().to(device)\n",
        "    # }),\n",
        "    # \"BERT\": nn.ModuleDict({\n",
        "    #     \"model\": BERT(n_classes=2).to(device),\n",
        "    #     \"loss_fn\": nn.CrossEntropyLoss().to(device)\n",
        "    # }),\n",
        "    \"biLSTM\": nn.ModuleDict({\n",
        "        \"model\": biLSTM(h_dim=100, lstm_layers=2).to(device),\n",
        "        \"loss_fn\": nn.CrossEntropyLoss().to(device)\n",
        "    }),\n",
        "  }\n",
        "\n",
        "  %%time\n",
        "\n",
        "  for modelName in models.keys():\n",
        "\n",
        "      print(f\"________ MODEL: {modelName} ________\")\n",
        "      print(''.join(r_config))\n",
        "      ### Model Unpacking\n",
        "      m = models[modelName]\n",
        "      model = m[\"model\"]\n",
        "      loss_fn = m[\"loss_fn\"]\n",
        "      # optimizer = AdamW(model.parameters(), lr=2e-3, correct_bias=False)\n",
        "      optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "      print(f\"________ {modelName}________\")\n",
        "\n",
        "          # Total Steps & Scheduler\n",
        "      total_steps = len(train_data_loader) * EPOCHS\n",
        "      scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "      )\n",
        "      \n",
        "      # Parameters\n",
        "      params = {\n",
        "          \"modelName\": modelName,\n",
        "          \"data_source\": data_source,\n",
        "          \"model\": model,\n",
        "          \"loss_fn\": loss_fn,\n",
        "          \"optimizer\": optimizer,\n",
        "          \"scheduler\": scheduler,\n",
        "          \"device\": device,\n",
        "          \"train_data_loader\": train_data_loader,\n",
        "          \"val_data_loader\": val_dls,\n",
        "          \"df_train\" :df_train,\n",
        "          \"df_vals\": df_vals,\n",
        "          \"df_tests\": df_tests,\n",
        "          \"run_config\": r_config\n",
        "      }\n",
        "\n",
        "      run_epochs(params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJPs8zTNG-F9",
        "outputId": "4c33c929-7c9b-403a-e457-cc57aa107440"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
            "Wall time: 5.96 µs\n",
            "________ MODEL: biLSTM ________\n",
            "ety\n",
            "________ biLSTM________\n",
            "Epoch 1/10\n",
            "----------\n",
            "Train loss 0.631632815917749 accuracy 0.6410894330234386\n",
            "Youtube\n",
            "Validation loss 0.5376977396011352 accuracy 0.770408163265306 f1 0.7058823529411764\n",
            "\n",
            "Email\n",
            "Validation loss 0.7670732488999 accuracy 0.5458937198067633 f1 0.11654135338345864\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.5558534838755925 accuracy 0.7568922305764411 f1 0.7000000000000001\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train loss 0.4605181703392748 accuracy 0.8488935445855703\n",
            "Youtube\n",
            "Validation loss 0.6019296610355377 accuracy 0.7066326530612245 f1 0.63258785942492\n",
            "\n",
            "Email\n",
            "Validation loss 0.5358895925375131 accuracy 0.7777777777777778 f1 0.4010416666666667\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.39195717493693033 accuracy 0.9206349206349206 f1 0.9180327868852459\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train loss 0.39167328214770214 accuracy 0.9205185282178867\n",
            "Youtube\n",
            "Validation loss 0.5730501163005829 accuracy 0.7346938775510203 f1 0.6623376623376624\n",
            "\n",
            "Email\n",
            "Validation loss 0.49707161692472607 accuracy 0.8164251207729468 f1 0.5432692307692307\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.3781930347283681 accuracy 0.9352548036758562 f1 0.9314462627156126\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train loss 0.3689390400941459 accuracy 0.9441534634018595\n",
            "Youtube\n",
            "Validation loss 0.5540050494670868 accuracy 0.760204081632653 f1 0.6987179487179488\n",
            "\n",
            "Email\n",
            "Validation loss 0.5357573390007019 accuracy 0.7777777777777778 f1 0.3882978723404255\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.381388599673907 accuracy 0.931077694235589 f1 0.9261083743842365\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train loss 0.3637763288008605 accuracy 0.9489982977609009\n",
            "Youtube\n",
            "Validation loss 0.5190833210945129 accuracy 0.7959183673469387 f1 0.7660818713450294\n",
            "\n",
            "Email\n",
            "Validation loss 0.5030072418543009 accuracy 0.808695652173913 f1 0.5147058823529411\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.3789998471736908 accuracy 0.9344193817878028 f1 0.9314110965487112\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train loss 0.35323132104898625 accuracy 0.9592772030902187\n",
            "Youtube\n",
            "Validation loss 0.5043515360355377 accuracy 0.8061224489795917 f1 0.7840909090909091\n",
            "\n",
            "Email\n",
            "Validation loss 0.4875507024618296 accuracy 0.8241545893719806 f1 0.5727699530516431\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.37602556685606636 accuracy 0.9365079365079365 f1 0.9344827586206895\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train loss 0.3445082315911797 accuracy 0.9687049888699751\n",
            "Youtube\n",
            "Validation loss 0.5142618644237519 accuracy 0.7857142857142857 f1 0.7572254335260116\n",
            "\n",
            "Email\n",
            "Validation loss 0.49165849456420313 accuracy 0.8202898550724638 f1 0.5571428571428572\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.37652164975802105 accuracy 0.9369256474519632 f1 0.9343763581051716\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train loss 0.33973540766701027 accuracy 0.9733534110252717\n",
            "Youtube\n",
            "Validation loss 0.49981539487838744 accuracy 0.8086734693877551 f1 0.780058651026393\n",
            "\n",
            "Email\n",
            "Validation loss 0.4985569660480206 accuracy 0.8144927536231884 f1 0.5339805825242718\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.3785073804855347 accuracy 0.935672514619883 f1 0.9325744308231173\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train loss 0.33966276143233814 accuracy 0.9736152939635984\n",
            "Youtube\n",
            "Validation loss 0.4796694540977478 accuracy 0.8290816326530611 f1 0.8154269972451791\n",
            "\n",
            "Email\n",
            "Validation loss 0.4505396320269658 accuracy 0.859903381642512 f1 0.6854663774403471\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.38021871288617454 accuracy 0.9327485380116959 f1 0.9297863061491496\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train loss 0.34203018302692795 accuracy 0.9709964645803326\n",
            "Youtube\n",
            "Validation loss 0.4721928095817566 accuracy 0.8392857142857142 f1 0.8355091383812011\n",
            "\n",
            "Email\n",
            "Validation loss 0.4064279684653649 accuracy 0.9053140096618357 f1 0.80859375\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.38784789899984995 accuracy 0.9252297410192146 f1 0.9232089232089232\n",
            "\n",
            "best f1: 0.8557706038633749, accuracy: 0.8899431549889215\n",
            "test f1 score: 0.3365384615384615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
            "Wall time: 5.48 µs\n",
            "________ MODEL: biLSTM ________\n",
            "sty\n",
            "________ biLSTM________\n",
            "Epoch 1/10\n",
            "----------\n",
            "Train loss 0.45410959366040354 accuracy 0.8554664956716896\n",
            "Youtube\n",
            "Validation loss 0.654227420091629 accuracy 0.6683673469387754 f1 0.5608108108108109\n",
            "\n",
            "SMS\n",
            "Validation loss 0.35975114830902644 accuracy 0.9488789237668162 f1 0.8118811881188118\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.38121841291586556 accuracy 0.9314954051796156 f1 0.9278169014084507\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train loss 0.37817209518872774 accuracy 0.9354921449182431\n",
            "Youtube\n",
            "Validation loss 0.5580805265903472 accuracy 0.7576530612244897 f1 0.7164179104477612\n",
            "\n",
            "SMS\n",
            "Validation loss 0.34783447086811065 accuracy 0.9641255605381166 f1 0.8657718120805369\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.39847437222798665 accuracy 0.9147869674185463 f1 0.9135593220338983\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train loss 0.3621262842875261 accuracy 0.9508175697338891\n",
            "Youtube\n",
            "Validation loss 0.6077006363868713 accuracy 0.7066326530612245 f1 0.637223974763407\n",
            "\n",
            "SMS\n",
            "Validation loss 0.34376731046608516 accuracy 0.968609865470852 f1 0.8717948717948718\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.3888813191652298 accuracy 0.923141186299081 f1 0.919085312225154\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train loss 0.35538531529597744 accuracy 0.9578069894196859\n",
            "Youtube\n",
            "Validation loss 0.5386022686958313 accuracy 0.7806122448979591 f1 0.7485380116959065\n",
            "\n",
            "SMS\n",
            "Validation loss 0.3301849339689527 accuracy 0.9829596412556054 f1 0.930909090909091\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.37227189441521963 accuracy 0.9398496240601504 f1 0.936897458369851\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train loss 0.3480179090377612 accuracy 0.9647964091054826\n",
            "Youtube\n",
            "Validation loss 0.5567340695858002 accuracy 0.7551020408163265 f1 0.7192982456140351\n",
            "\n",
            "SMS\n",
            "Validation loss 0.34601957287107193 accuracy 0.967713004484305 f1 0.8723404255319148\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.3860994899272919 accuracy 0.9256474519632414 f1 0.921169176262179\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train loss 0.34698354730239284 accuracy 0.9662712407823021\n",
            "Youtube\n",
            "Validation loss 0.5105555558204651 accuracy 0.8035714285714285 f1 0.7830985915492957\n",
            "\n",
            "SMS\n",
            "Validation loss 0.3380206269877298 accuracy 0.9739910313901345 f1 0.8975265017667844\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.37634307821591695 accuracy 0.9360902255639098 f1 0.9341935483870969\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train loss 0.34036154361871573 accuracy 0.9728117986534146\n",
            "Youtube\n",
            "Validation loss 0.5108739066123963 accuracy 0.8035714285714285 f1 0.7793696275071632\n",
            "\n",
            "SMS\n",
            "Validation loss 0.3356182745524815 accuracy 0.9775784753363229 f1 0.9077490774907749\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.37003395676612855 accuracy 0.9423558897243107 f1 0.9403114186851211\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train loss 0.33741773990484386 accuracy 0.9757614620070536\n",
            "Youtube\n",
            "Validation loss 0.4877037262916565 accuracy 0.8265306122448979 f1 0.8100558659217878\n",
            "\n",
            "SMS\n",
            "Validation loss 0.33586484151227136 accuracy 0.9775784753363229 f1 0.9056603773584904\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.36776970128218334 accuracy 0.9448621553884711 f1 0.9425587467362924\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train loss 0.3362465777152624 accuracy 0.9769798012183393\n",
            "Youtube\n",
            "Validation loss 0.4458872854709625 accuracy 0.8698979591836734 f1 0.8640000000000001\n",
            "\n",
            "SMS\n",
            "Validation loss 0.3319679064410073 accuracy 0.9811659192825112 f1 0.9219330855018588\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.37486716945966087 accuracy 0.9373433583959899 f1 0.9354561101549053\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train loss 0.3376921129532349 accuracy 0.9755690926579033\n",
            "Youtube\n",
            "Validation loss 0.4338945150375366 accuracy 0.8826530612244897 f1 0.8795811518324608\n",
            "\n",
            "SMS\n",
            "Validation loss 0.3311534038611821 accuracy 0.9820627802690584 f1 0.9264705882352942\n",
            "\n",
            "Twitter\n",
            "Validation loss 0.37543089588483175 accuracy 0.9373433583959899 f1 0.9358426005132592\n",
            "\n",
            "best f1: 0.9139647801936714, accuracy: 0.9340197332965127\n",
            "test f1 score: 0.4148681055155875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DpE54TyoF2nq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}